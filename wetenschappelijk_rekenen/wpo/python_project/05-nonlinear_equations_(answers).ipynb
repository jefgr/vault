{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sessie 5 - Niet-lineaire vergelijkingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voer de onderstaande code uit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_performance(x, y):\n",
    "    plt.xlabel('Parameter')\n",
    "    plt.ylabel('Performance')\n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter optimalisatie\n",
    "\n",
    "Sommige methodes kunnen afhankelijk van hun parametrische instellingen beter of slechter werken (b.v., predictie algoritmen, routing in netwerken, load balancing en simulators).\n",
    "'Parameter tuning' is dus een essentieel onderdeel bij de applicatie van zulke algoritmes.\n",
    "\n",
    "Een bijkomende limitatie is dat sommige algoritmes te lang duren om extensief de optimale parameter te zoeken.\n",
    "Om dit probleem aan te pakken, kunnen we enkele equidistante parameters (<code>data_x</code>) bepalen, en zo een benaderende functie proberen te bepalen die de bekomen performanties (<code>data_y</code>) verklaart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]\n",
    "data_y = [10., 10.84147098, 11.81859485, 10.42336002, 6.97279002, 5.20537863,\n",
    "          8.32350701, 14.59890619, 17.91486597, 13.70906637, 4.55978889]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hoe we zo'n benaderende functie kunnen bepalen (regressie), zien we in een latere sessie, maar op het moment is deze gegeven (<code>f(x)</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.py\n",
    "\n",
    "# Regression model\n",
    "f = create_model(data_x, data_y)\n",
    "\n",
    "# Plot 100 points and their values according to the model f.\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = list(map(f, x))\n",
    "plt.plot(data_x, data_y, 'r.', markersize=10)\n",
    "plot_performance(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze functie heeft geen expliciete formule gegeven. Het enige wat we weten, is dat deze functie minstens twee keer differentieerbaar is. Gegeven deze functie <code>f(x)</code> over het interval $x \\in [0, 10]$, kan je het extremum (en dus de optimale parameter) vinden? Welke stappen zou je kunnen volgen om een oplossing te bekomen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zoek de afgeleides\n",
    "\n",
    "Aangezien we geen expliciete functie hebben, moeten we de afgeleide benaderen. Gebruik hiervoor Taylor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ANTWOORD:</b>\n",
    "\n",
    "Eerste orde Taylor-expansie: $$f(y) \\approx f(a) + f(a)'(y-a)$$\n",
    "Dus: $$f(x+h) \\approx f(x) + f(x)'(x+h-x) \\Leftrightarrow f(x)' \\approx \\frac{f(x+h) - f(x)}{h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive(f, x, h):\n",
    "    \"\"\"\n",
    "    Approximate the derivative of the function f in x.\n",
    "    The approximation goes towards the actual derivative when h goes towards zero.\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "\n",
    "# Define the derivative of f\n",
    "h = 1e-5\n",
    "df = lambda x: derive(f, x, h)\n",
    "\n",
    "# Plot the performance over 100 points.\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = list(map(df, x))\n",
    "plot_performance(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gegeven de eerste afgeleide, kan je met deze methode ook de tweede afgeleide berekenen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Methode van Newton\n",
    "\n",
    "Implementeer de methode van Newton en ontwerp een algoritme die een extremum vindt in een gegeven functie m.b.v. Newton.\n",
    "Dit algoritme heet het Gauss-Newton algoritme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, df, a, tol):\n",
    "    \"\"\"\n",
    "    Compute the root of f using Newton's algorithm starting at 'a'.\n",
    "    The procedure is stopped when the error reaches 'tol'.\n",
    "    \"\"\"\n",
    "    error = float(\"+inf\")\n",
    "    while error > tol:\n",
    "        x = a\n",
    "        if df(x) == 0:\n",
    "            error = 0\n",
    "        else:\n",
    "            a = x - f(x) / df(x)\n",
    "            error = abs(x - a)\n",
    "    return a\n",
    "\n",
    "def find_extremum_newton(f, a, tol):\n",
    "    \"\"\"\n",
    "    Find an extremum in f starting from the initial guess a.\n",
    "    The procedure is stopped when the error reaches 'tol'.\n",
    "    \"\"\"\n",
    "    h = 1e-5\n",
    "    df = lambda x: derive(f, x, h)\n",
    "    ddf = lambda x: derive(df, x, h)\n",
    "    return newton(df, ddf, a, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "tol = 1e-10\n",
    "initial_guess = 7\n",
    "zero = find_extremum_newton(f, initial_guess, tol)\n",
    "epsilon = 1e-3\n",
    "print(\"Optimale parameter z: \", zero)\n",
    "print(\"Hoogste performantie f(z): \", f(zero))\n",
    "print(\"Lokale waarden naast het extremum: links -\", f(zero - epsilon), ' / rechts - ', f(zero + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lokale waarden hierboven zouden kleiner moeten zijn dan het gevonden extremum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisatie newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_vis(f, df, a, tol):\n",
    "    \"\"\"\n",
    "    Compute the root of f using Newton's algorithm starting at 'a'.\n",
    "    The procedure is stopped when the error reaches 'tol'.\n",
    "    \"\"\"\n",
    "    error = float(\"+inf\")\n",
    "    guess = []\n",
    "    while error > tol:\n",
    "        guess.append(a)\n",
    "        x = a\n",
    "        if df(x) == 0:\n",
    "            error = 0\n",
    "        else:\n",
    "            a = x - f(x) / df(x)\n",
    "            error = abs(x - a)\n",
    "    return a, guess\n",
    "\n",
    "def find_extremum_vis(f, a, tol):\n",
    "    \"\"\"\n",
    "    Find an extremum in f starting from the initial guess a.\n",
    "    The procedure is stopped when the error reaches 'tol'.\n",
    "    \"\"\"\n",
    "    h = 1e-5\n",
    "    df = lambda x: derive(f, x, h)\n",
    "    ddf = lambda x: derive(df, x, h)\n",
    "    return newton_vis(df, ddf, a, tol)\n",
    "\n",
    "def visualise():\n",
    "    tol = 1e-10\n",
    "    initial_guess = 7\n",
    "    zero, guess = find_extremum_vis(f, initial_guess, tol)\n",
    "    epsilon = 1e-3\n",
    "    print(\"Optimale parameter z: \", zero)\n",
    "    #print(\"Hoogste performantie f(z): \", f(zero))\n",
    "    #print(\"Lokale waarden naast het extremum: links -\", f(zero - epsilon), ' / rechts - ', f(zero + epsilon))\n",
    "\n",
    "    # Plot 100 points and their values according to the model f.\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = list(map(f, x))\n",
    "    data_y = list(map(f, guess))\n",
    "    # add labels to each point in the plot, where the label is the index of the point\n",
    "    # For example, the red point with label 0 corresponds to the initial guess\n",
    "    for i in range(len(guess)):\n",
    "        plt.text(guess[i], data_y[i], str(i))\n",
    "    plt.plot(guess, data_y, 'r.', markersize=10)\n",
    "    plot_performance(x, y)\n",
    "    \n",
    "visualise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Tweede mogelijkheid\n",
    "\n",
    "De geconstrueerde methode geeft een benadering op verschillende niveaus:<br>\n",
    "\n",
    "* Newton geeft een benadering op de wortel\n",
    "* Taylor geeft een benadering op de tweede afgeleide ($f''(x) \\approx f''(\\xi)$ met $\\xi \\in [x, x+h]$)\n",
    "* Taylor geeft een benadering op de eerste afgeleide ($f'(x) \\approx f'(\\xi)$ met $\\xi \\in [x, x+h]$)\n",
    "* <code>f(x)</code> is een model van de effectieve onderliggende functie\n",
    "\n",
    "Met de methode van Steffensen, zou je de tweede afgeleide niet moeten benaderen.\n",
    "Implementeer de methode van Steffensen om het optimalisatie probleem op te lossen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(f, a, tol):\n",
    "    \"\"\"\n",
    "    Find the root of f starting from 'a'.\n",
    "    The procedure is stopped when the error reaches 'tol'.\n",
    "    \"\"\"\n",
    "    g = lambda x: (f(x + f(x)) - f(x)) / f(x)\n",
    "    error = float(\"+inf\")\n",
    "    while error > tol:\n",
    "        x = a\n",
    "        if np.isnan(g(x)): # or if f(x) == 0:\n",
    "            error = 0\n",
    "        else:\n",
    "            a = x - f(x) / g(x)\n",
    "            error = abs(x - a)\n",
    "    return a\n",
    "\n",
    "def find_extremum_stef(f, a, tol):\n",
    "    \"\"\"\n",
    "    Find an extremum in f starting from a.\n",
    "    The procedure is stopped when the error reaches 'tol'.\n",
    "    \"\"\"\n",
    "    h = 1e-5\n",
    "    df = lambda x: derive(f, x, h)\n",
    "    return find_root(df, a, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "tol = 1e-10\n",
    "zero = find_extremum_stef(f, 8, tol)\n",
    "epsilon = 1e-3\n",
    "print(\"Optimale parameter z: \", zero)\n",
    "print(\"Hoogste performantie f(z): \", f(zero))\n",
    "print(\"Lokale waarden naast het extremum: links -\", f(zero - epsilon), ' / rechts - ', f(zero + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
